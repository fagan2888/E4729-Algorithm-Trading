{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series demo 1: AR(p) models on synthetic data, with known parameters\n",
    "\n",
    "\n",
    "**Guest lecture**\n",
    "\n",
    "Columbia IEOR 4729 : _Model Based Trading: Theory and Practice_\n",
    "\n",
    "Q McCallum (http://qethanm.cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed( 4729 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use some `statsmodels` builtins to generate synthetic data that fits an autoregressive AR(3) model with the parameters: `0.5` , `0.25` , `-0.1`\n",
    "\n",
    "Of note:\n",
    "\n",
    "- Since we're using an ARMA generator, we only pass in parameters for the autoregressive model (and leave the params for the moving average model blank)\n",
    "- Because of how `ArmaProcess` works, we pass in _negative_ values of our AR parameters.  This is only for AR parameters, not MA parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_ar3 = sm.tsa.ArmaProcess(\n",
    "    ar = [ 1 , - 0.5 , - 0.25 , 0.1 ] ,\n",
    "    ma = [ 1 ] ,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let the ARMA process warm up for a bit (`burnin` parameter)\n",
    "## and generate 500 points befitting an AR(3) model of our stated parameters\n",
    "y_ar3 = process_ar3.generate_sample(\n",
    "    500 ,\n",
    "    burnin = 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## being lazy and building a temporary Series for plotting\n",
    "_ = pd.Series( y_ar3 ).plot(\n",
    "    title = \"AR(3) data\" ,\n",
    "    figsize = ( 20 , 6 )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know that this is an AR(3) model (with a little noise)\n",
    "which gives us the chance to test our tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## acf -- autocorrelation function plot, aka \"correlogram\"\n",
    "## Correlation ofthe series, against itself, at different lags.\n",
    "## The light-blue area represents a confidence interval of 90% or 95%\n",
    "## (depending on settings) ; any points outside of that range\n",
    "## _might_ be significant.\n",
    "\n",
    "\n",
    "_ = sm.graphics.tsa.plot_acf( y_ar3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pacf -- partial autocorrelation function plot\n",
    "## Similar to autocorrelation function, but removes correlations based on previous lags.\n",
    "## Once again, points outside of the shaded blue area _might_ be significant.\n",
    "\n",
    "## For a deeper explanation of ACF vs PACF, see:\n",
    "## https://towardsdatascience.com/significance-of-acf-and-pacf-plots-in-time-series-analysis-2fa11a5d10a8\n",
    "\n",
    "_ = sm.graphics.tsa.plot_pacf( y_ar3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## That's the data, and that hints at an AR(3) model.  Maybe.  Let's try to fit a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the `(3,0)` is a tuple that reflects the order of the AR and MA portions,\n",
    "## respectively.  Since this is an AR(3) model that means MA(0), so, we pass `(3,0)`\n",
    "\n",
    "model_ar3 = sm.tsa.ARMA( y_ar3 , (3,0) )\n",
    "fit_ar3 = model_ar3.fit( trend=\"nc\" , disp=0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_ar3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for a fit:\n",
    "## -  .fittedvalues --> values as generated by this set of params\n",
    "## -  .resid --> residuals, aka \"original data minus fitted values\"\n",
    "\n",
    "## and:\n",
    "## - y_ar3 --> the original, synthetic data from the series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pd.DataFrame(\n",
    "    {\n",
    "        \"y_ar\"   : y_ar3 , \n",
    "        \"model\"  : fit_ar3.fittedvalues\n",
    "    }\n",
    ").plot(\n",
    "    title = \"AR(3) series: reality (y_ar) vs prediction (model)\" ,\n",
    "    figsize = ( 20 , 6 )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## So far, so good ... but ... remember the rule: check your residuals.\n",
    "_ = sm.graphics.tsa.plot_acf( fit_ar3.resid )\n",
    "_ = sm.graphics.tsa.plot_pacf( fit_ar3.resid )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shape on an ACF is a strong hint of white noise: notice, we only get a meaningful correlation at lag 0, which will always be 1.\n",
    "\n",
    "Since it looks like a lot of random noise /white noise from here ... we're good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit's `summary()` method included some diagnostic values.  Those are useful when _comparing to other fits._  On their own, they don't say a whole lot.\n",
    "\n",
    "The big ones are:\n",
    "\n",
    "- **AIC:** Akaike Information Criterion -- judges information loss in a model, based on the likelihood function; penalizes for number of parameters (to avoid overfitting)\n",
    "- **BIC:** Bayesian Information Criterion -- similar to AIC, but penalizes differently for number of terms\n",
    "\n",
    "\n",
    "We can test this.  Let's say we _didn't_ know, _a priori,_ that this was an AR(3) model. \n",
    "\n",
    "One way to check would be to try a handful of other fits.  We can use a `for` loop for this, and compare the AIC and BIC values that come back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We already know this data reflects AR(3) but we'll show a simple way to\n",
    "## test when we believe a series to be of _some_ kind of AR form:\n",
    "\n",
    "ar_p_to_try = [\n",
    "    (5,0) ,\n",
    "    (4,0) ,\n",
    "    (3,0) ,\n",
    "    (2,0) ,\n",
    "    (1,0) ,\n",
    "]\n",
    "\n",
    "param_search_results = []\n",
    "\n",
    "print( \"(Remember: lowest AIC wins)\" )\n",
    "\n",
    "for ar_p in ar_p_to_try :\n",
    "    print( \"trying parameters: {}\".format( ar_p ) )\n",
    "    model_testing = sm.tsa.ARMA( y_ar3 , ar_p ).fit( trend=\"nc\" , disp=0 )\n",
    "    ## model_testing = sm.tsa.AR( y_ar ).fit( maxlag=10 )\n",
    "\n",
    "    print( \"model params: {}\".format( model_testing.params ) )\n",
    "    print( \"AIC:     {}\".format( model_testing.aic ) )\n",
    "    print( \"BIC:     {}\".format( model_testing.bic ) )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the AR(3) model -- parameters `(3,0)` -- has the lowest AIC and BIC, which means that it's the best fit.\n",
    "\n",
    "That doesn't mean that the lowest fit is the _absolute_ best possible model; it just means that it's the best for the values we tried.  It's entirely possible that a different model altogether would be a better fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
